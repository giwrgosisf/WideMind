<!DOCTYPE html>
<html lang="en">
<!-- Page title -->

<head>
    <title>Understanding the Linux Kernel</title>
    <meta name="description" content="Page describing the creators and the vision of widemind">
    <meta name="keywords"
        content="e-learning, computer science, machine learning, neural networks,operating systems, system organization, books, video courses">
    <meta name="author" content="WideMind Organization">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="../Resources/website-icon.png" rel="shortcut icon" type="image/png" />
</head>

<!-- Start of the main body -->

<body>
    <header>
        <img src="../Resources/widemind-logo.png">
        <form id="search-bar">
            <input name="search-input" type="text" autocomplete="off" spellcheck="false"
                placeholder="Search for books or recordings...">
            <button type="button">
                <img src="../Resources/search-icon.svg">
            </button>
        </form>
        <nav>
            <ul>
                <li><a href="index.html">Home</a></li>
                <li><a href="categories.html">Categories</a></li>
                <li><a href="aboutUs.html">About Us</a></li>
            </ul>
        </nav>
    </header>
    <main>
        <h1>Understanding the Linux Kernel</h1>
        <a href="operatingSystems.html">
            <p>Go back</p>
        </a>
        <p>Authors: Daniel P. Bovet, Marco Cesati</p>
        <p>Date of publismnet: November 17, 2005</p>
        <img src="../Resources/Understanding the Linux Kernel.jpg" width="318" height="409">
        <div class="stars-rating">
            <span>★</span>
            <span>★</span>
            <span>★</span>
            <span>★</span>
            <span>☆</span>
            <span>
                <p><strong>4.12</strong></p>
            </span>
            <span>
                <p>711 ratings</p>
            </span>
        </div>
        <p>Price: €38.79</p>
        <p>Pages: 944</p>


        <p>In order to thoroughly understand what makes Linux tick and why it works so well on a wide variety of
            systems, you need to delve deep into the heart of the kernel. The kernel handles all interactions between
            the CPU and the external world, and determines which programs will share processor time, in what order. It
            manages limited memory so well that hundreds of processes can share the system efficiently, and expertly
            organizes data transfers so that the CPU isn't kept waiting any longer than necessary for the relatively
            slow disks.

            The third edition of Understanding the Linux Kernel takes you on a guided tour of the most significant data
            structures, algorithms, and programming tricks used in the kernel. Probing beyond superficial features, the
            authors offer valuable insights to people who want to know how things really work inside their machine.
            Important Intel-specific features are discussed. Relevant segments of code are dissected line by line. But
            the book covers more than just the functioning of the code; it explains the theoretical underpinnings of why
            Linux does things the way it does.

            This edition of the book covers Version 2.6, which has seen significant changes to nearly every kernel
            subsystem, particularly in the areas of memory management and block devices. The book focuses on the
            following topics:

            Memory management, including file buffering, process swapping, and Direct memory Access (DMA) The Virtual
            Filesystem layer and the Second and Third Extended Filesystems Process creation and scheduling Signals,
            interrupts, and the essential interfaces to device drivers Timing Synchronization within the kernel
            Interprocess Communication (IPC) Program execution
            Understanding the Linux Kernel will acquaint you with all the inner workings of Linux, but it's more than
            just an academic exercise. You'll learn what conditions bring out Linux's best performance, and you'll see
            how it meets the challenge of providing good system response during process scheduling, file access, and
            memory management in a wide variety of environments. This book will help you make the most of your Linux
            system
        </p>

        <p>Bovet, D. P., Cesati, M. (2005). Understanding the Linux Kernel. United States: O'Reilly Media.</p>



        <h2 id="Table of contents">Table of contents</h2>

        <table>
            <tr>
                <th align="left"><a href="#PREFACE">PREFACE</a></th>
            </tr>
            <tr>
                <th align="left"><a href="#INTRODUCTION">1 INTRODUCTION</a></th>
            </tr>
            <tr>
                <th align="left"><a href="#MEMORY ADDRESSING">2 MEMORY ADDRESSING</a></th>
            </tr>
            <tr>
                <th align="left"><a href="#PROCESSES">3 PROCESSES</a></th>
            </tr>
            <tr>
                <th align="left"><a href="#INTERRUPTS AND EXCEPTIONS">4 INTERRUPTS AND EXCEPTIONS</a></th>
            </tr>
            <tr>
                <th align="left"><a href="#KERNEL SYNCHRONIZATION">5 KERNEL SYNCHRONIZATION</a></th>
            </tr>
            <tr>
                <th align="left"><a href="#TIMING MEASUREMENTS">6 TIMING MEASUREMENTS</a></th>
            </tr>
            <tr>
                <th align="left"><a href="#PROCESS SCHEDULING">7 PROCESS SCHEDULING"</a></th>
            </tr>
            <tr>
                <th align="left"><a href="#MEMORY MANAGEMENT">8 MEMORY MANAGEMENT</a></th>
            </tr>
            <tr>
                <th align="left"><a href="#PROCESS ADDRESS SPACE">9 PROCESS ADDRESS SPACE</a></th>
            </tr>
            <tr>
                <th align="left"><a href="#SYSTEM CALLS">10 SYSTEM CALLS</a></th>
            </tr>
            <tr>
                <th align="left"><a href="#SIGNALS">11 SIGNALS</a></th>
            </tr>
            <tr>
                <th align="left"><a href="#THE VIRTUAL FILESYSTEM">12 THE VIRTUAL FILESYSTEM</a></th>
            </tr>
            <tr>
                <th align="left"><a href="#I/O ARCHITECTURE AND DEVICE DRIVERS">13 I/O ARCHITECTURE AND DEVICE DRIVERS"
                    </a></th>
            </tr>
            <tr>
                <th align="left"><a href="#BLOCK DEVICE DRIVERS">14 BLOCK DEVICE DRIVERS</a></th>
            </tr>
            <tr>
                <th align="left"><a href="#THE PAGE CACHE">15 THE PAGE CACHE</a></th>
            </tr>
            <tr>
                <th align="left"><a href="#ACCESSING FILES">16 ACCESSING FILES</a></th>
            </tr>
            <tr>
                <th align="left"><a href="#PAGE FRAME RECLAIMING">17 PAGE FRAME RECLAIMING</a></th>
            </tr>
            <tr>
                <th align="left"><a href="#THE EXT2 AND EXT3 FILESYSTEMS">18 THE EXT2 AND EXT3 FILESYSTEMS</a></th>
            </tr>
            <tr>
                <th align="left"><a href="#PROCESS COMMUNICATION">19 PROCESS COMMUNICATION</a></th>
            </tr>
            <tr>
                <th align="left"><a href="#PROGRAM EXECUTION">20 PROGRAM EXECUTION</a></th>
            </tr>
            <tr>
                <th align="left"><a href="#SYSTEM STARTUP"> SYSTEM STARTUP</a></th>
            </tr>
            <tr>
                <th align="left"><a href="#MODULES"> MODULES</a></th>
            </tr>
            <tr>
                <th align="left"><a href="#BIBLIOGRAPHY"> BIBLIOGRAPHY</a></th>
            </tr>
        </table>
        <br>
        <br>
        <section id="PREFACE">
            <details>
                <summary>PREFACE</summary>
                <p>
                    In the spring semester of 1997, we taught a course on operating systems based on
                    Linux 2.0. The idea was to encourage students to read the source code. To achieve
                    this, we assigned term projects consisting of making changes to the kernel and performing
                    tests on the modified version. We also wrote course notes for our students
                    about a few critical features of Linux such as task switching and task scheduling.
                    Out of this work—and with a lot of support from our O’Reilly editor Andy Oram—
                    came the first edition of Understanding the Linux Kernel at the end of 2000, which
                    covered Linux 2.2 with a few anticipations on Linux 2.4. The success encountered by
                    this book encouraged us to continue along this line. At the end of 2002, we came out
                    with a second edition covering Linux 2.4. You are now looking at the third edition,
                    which covers Linux 2.6.
                    As in our previous experiences, we read thousands of lines of code, trying to make
                    sense of them. After all this work, we can say that it was worth the effort. We learned
                    a lot of things you don’t find in books, and we hope we have succeeded in conveying
                    some of this information in the following pages.
                </p>
                <a href="#Table of contents">Return to the table of contents</a>
            </details>
        </section>
        <section id="INTRODUCTION">
            <details>
                <summary>INTRODUCTION</summary>
                <p>Linux is a member of the large family of Unix-like operating systems. A relative newcomer
                    experiencing sudden spectacular popularity starting in the late 1990s, Linux
                    joins such well-known commercial Unix operating systems as System V Release 4
                    (SVR4), developed by AT&T (now owned by the SCO Group); the 4.4 BSD release
                    from the University of California at Berkeley (4.4BSD); Digital UNIX from Digital
                    Equipment Corporation (now Hewlett-Packard); AIX from IBM; HP-UX from
                    Hewlett-Packard; Solaris from Sun Microsystems; and Mac OS X from Apple Computer,
                    Inc. Beside Linux, a few other opensource Unix-like kernels exist, such as
                    FreeBSD, NetBSD, and OpenBSD.
                    Linux was initially developed by Linus Torvalds in 1991 as an operating system for
                    IBM-compatible personal computers based on the Intel 80386 microprocessor. Linus
                    remains deeply involved with improving Linux, keeping it up-to-date with various
                    hardware developments and coordinating the activity of hundreds of Linux developers
                    around the world. Over the years, developers have worked to make Linux available
                    on other architectures, including Hewlett-Packard’s Alpha, Intel’s Itanium,
                    AMD’s AMD64, PowerPC, and IBM’s zSeries.
                    One of the more appealing benefits to Linux is that it isn’t a commercial operating
                    system: its source code under the GNU General Public License (GPL)† is open and
                    available to anyone to study (as we will in this book); if you download the code (the
                    official site is http://www.kernel.org) or check the sources on a Linux CD, you will be
                    able to explore, from top to bottom, one of the most successful modern operating
                    systems. This book, in fact, assumes you have the source code on hand and can
                    apply what we say to your own explorations.
                    Technically speaking, Linux is a true Unix kernel, although it is not a full Unix operating
                    system because it does not include all the Unix applications, such as filesystem
                    utilities, windowing systems and graphical desktops, system administrator commands,
                    text editors, compilers, and so on. However, because most of these programs
                    are freely available under the GPL, they can be installed in every Linux-based system.
                    Because the Linux kernel requires so much additional software to provide a useful
                    environment, many Linux users prefer to rely on commercial distributions, available on
                    CD-ROM, to get the code included in a standard Unix system. Alternatively, the code
                    may be obtained from several different sites, for instance http://www.kernel.org. Several
                    distributions put the Linux source code in the /usr/src/linux directory. In the rest of
                    this book, all file pathnames will refer implicitly to the Linux source code directory</p>
            </details>
            </tr>
        </section>
        <section id="MEMORY ADDRESSING">
            <details>
                <summary>MEMORY ADDRESSING</summary>
                <p>This chapter deals with addressing techniques. Luckily, an operating system is not
                    forced to keep track of physical memory all by itself; today’s microprocessors include
                    several hardware circuits to make memory management both more efficient and
                    more robust so that programming errors cannot cause improper accesses to memory
                    outside the program.
                    As in the rest of this book, we offer details in this chapter on how 80 × 86 microprocessors
                    address memory chips and how Linux uses the available addressing circuits.
                    You will find, we hope, that when you learn the implementation details on Linux’s
                    most popular platform you will better understand both the general theory of paging
                    and how to research the implementation on other platforms.
                    This is the first of three chapters related to memory management; Chapter 8 discusses
                    how the kernel allocates main memory to itself, while Chapter 9 considers
                    how linear addresses are assigned to processes.</p>

                <a href="#Table of contents">Return to the table of contents</a>
            </details>
        </section>
        <section id="PROCESSES">
            <details>
                <summary>PROCESS</summary>
                <p>The concept of a process is fundamental to any multiprogramming operating system.
                    A process is usually defined as an instance of a program in execution; thus, if 16
                    users are running vi at once, there are 16 separate processes (although they can share
                    the same executable code). Processes are often called tasks or threads in the Linux
                    source code.
                    In this chapter, we discuss static properties of processes and then describe how process
                    switching is performed by the kernel. The last two sections describe how processes
                    can be created and destroyed. We also describe how Linux supports
                    multithreaded applications—as mentioned in Chapter 1, it relies on so-called lightweight
                    processes (LWP).</p>
                <a href="#Table of contents">Return to the table of contents</a>
            </details>
        </section>
        <section id="INTERRUPTS AND EXCEPTIONS">
            <details>
                <summary>INTERRUPTS AND EXCEPTIONS</summary>
                <p>An interrupt is usually defined as an event that alters the sequence of instructions
                    executed by a processor. Such events correspond to electrical signals generated by
                    hardware circuits both inside and outside the CPU chip.
                    Interrupts are often divided into synchronous and asynchronous interrupts:
                    • Synchronous interrupts are produced by the CPU control unit while executing
                    instructions and are called synchronous because the control unit issues them
                    only after terminating the execution of an instruction.
                    • Asynchronous interrupts are generated by other hardware devices at arbitrary
                    times with respect to the CPU clock signals.
                    Intel microprocessor manuals designate synchronous and asynchronous interrupts as
                    exceptions and interrupts, respectively. We’ll adopt this classification, although we’ll
                    occasionally use the term “interrupt signal” to designate both types together (synchronous
                    as well as asynchronous).
                    Interrupts are issued by interval timers and I/O devices; for instance, the arrival of a
                    keystroke from a user sets off an interrupt.
                    Exceptions, on the other hand, are caused either by programming errors or by anomalous
                    conditions that must be handled by the kernel. In the first case, the kernel handles
                    the exception by delivering to the current process one of the signals familiar to
                    every Unix programmer. In the second case, the kernel performs all the steps needed
                    to recover from the anomalous condition, such as a Page Fault or a request—via an
                    assembly language instruction such as int or sysenter—for a kernel service.
                    We start by describing in the next section the motivation for introducing such signals.
                    We then show how the well-known IRQs (Interrupt ReQuests) issued by I/O
                    devices give rise to interrupts, and we detail how 80×86 processors handle interrupts
                    and exceptions at the hardware level. Then we illustrate, in the section “Initializing
                    the Interrupt Descriptor Table,” how Linux initializes all the data structures required by the 80×86
                    interrupt architecture. The remaining three sections describe
                    how Linux handles interrupt signals at the software level.
                    One word of caution before moving on: in this chapter, we cover only “classic”
                    interrupts common to all PCs; we do not cover the nonstandard interrupts of some
                    architectures.</p>
                <a href="#Table of contents">Return to the table of contents </a>
            </details>
        </section>
        <section id="KERNEL SYNCHRONIZATION">
            <details>
                <summary>KERNEL SYNCHRONIZATION</summary>
                <p>You could think of the kernel as a server that answers requests; these requests can
                    come either from a process running on a CPU or an external device issuing an interrupt
                    request. We make this analogy to underscore that parts of the kernel are not run
                    serially, but in an interleaved way. Thus, they can give rise to race conditions, which
                    must be controlled through proper synchronization techniques. A general introduction
                    to these topics can be found in the section “An Overview of Unix Kernels” in
                    Chapter 1.
                    We start this chapter by reviewing when, and to what extent, kernel requests are executed
                    in an interleaved fashion. We then introduce the basic synchronization primitives
                    implemented by the kernel and describe how they are applied in the most
                    common cases. Finally, we illustrate a few practical examples.</p>
                <a href="#Table of contents">Return to the table of contents</a>
            </details>
        </section>
        <section id="TIMING MEASUREMENTS">
            <details>
                <summary>TIMING MEASUREMENTS</summary>
                <p>Countless computerized activities are driven by timing measurements, often behind
                    the user’s back. For instance, if the screen is automatically switched off after you have
                    stopped using the computer’s console, it is due to a timer that allows the kernel to
                    keep track of how much time has elapsed since you pushed a key or moved the
                    mouse. If you receive a warning from the system asking you to remove a set of
                    unused files, it is the outcome of a program that identifies all user files that have not
                    been accessed for a long time. To do these things, programs must be able to retrieve a
                    timestamp identifying its last access time from each file. Such a timestamp must be
                    automatically written by the kernel. More significantly, timing drives process
                    switches along with even more visible kernel activities such as checking for time-outs.
                    We can distinguish two main kinds of timing measurement that must be performed
                    by the Linux kernel:
                    • Keeping the current time and date so they can be returned to user programs
                    through the time( ), ftime( ), and gettimeofday( ) APIs (see the section “The
                    time( ) and gettimeofday( ) System Calls” later in this chapter) and used by the
                    kernel itself as timestamps for files and network packets
                    • Maintaining timers—mechanisms that are able to notify the kernel (see the later
                    section “Software Timers and Delay Functions”) or a user program (see the later
                    sections “The setitimer( ) and alarm( ) System Calls” and “System Calls for
                    POSIX Timers”) that a certain interval of time has elapsed
                    Timing measurements are performed by several hardware circuits based on fixedfrequency
                    oscillators and counters. This chapter consists of four different parts. The
                    first two sections describe the hardware devices that underly timing and give an overall
                    picture of Linux timekeeping architecture. The following sections describe the
                    main time-related duties of the kernel: implementing CPU time sharing, updating
                    system time and resource usage statistics, and maintaining software timers. The last
                    section discusses the system calls related to timing measurements and the corresponding
                    service routines.</p>
                <a href="#Table of contents">Return to the table of contents</a>
            </details>
        </section>
        <section id="PROCESS SCHEDULING">
            <details>
                <summary>PROCESS SCHEDULING</summary>
                <p>Like every time sharing system, Linux achieves the magical effect of an apparent
                    simultaneous execution of multiple processes by switching from one process to
                    another in a very short time frame. Process switching itself was discussed in
                    Chapter 3; this chapter deals with scheduling, which is concerned with when to
                    switch and which process to choose.
                    The chapter consists of three parts. The section “Scheduling Policy” introduces the
                    choices made by Linux in the abstract to schedule processes. The section “The
                    Scheduling Algorithm” discusses the data structures used to implement scheduling
                    and the corresponding algorithm. Finally, the section “System Calls Related to
                    Scheduling” describes the system calls that affect process scheduling.
                    To simplify the description, we refer as usual to the 80 × 86 architecture; in particular,
                    we assume that the system uses the Uniform Memory Access model, and that the
                    system tick is set to 1 ms.</p>

                <a href="#Table of contents">Return to the table of contents</a>
            </details>
        </section>
        <section id="MEMORY MANAGEMENT"></section>
        <details>
            <summary>MEMORY MANAGEMENT</summary>
            <p>We saw in Chapter 2 how Linux takes advantage of 80 × 86’s segmentation and paging
                circuits to translate logical addresses into physical ones. We also mentioned that
                some portion of RAM is permanently assigned to the kernel and used to store both
                the kernel code and the static kernel data structures.
                The remaining part of the RAM is called dynamic memory. It is a valuable resource,
                needed not only by the processes but also by the kernel itself. In fact, the performance
                of the entire system depends on how efficiently dynamic memory is managed.
                Therefore, all current multitasking operating systems try to optimize the use of
                dynamic memory, assigning it only when it is needed and freeing it as soon as
                possible. Figure 8-1 shows schematically the page frames used as dynamic memory;
                see the section “Physical Memory Layout” in Chapter 2 for details.
                This chapter, which consists of three main sections, describes how the kernel allocates
                dynamic memory for its own use. The sections “Page Frame Management” and
                “Memory Area Management” illustrate two different techniques for handling physically
                contiguous memory areas, while the section “Noncontiguous Memory Area
                Management” illustrates a third technique that handles noncontiguous memory
                areas. In these sections we’ll cover topics such as memory zones, kernel mappings,
                the buddy system, the slab cache, and memory pools.</p>
            <a href="#Table of contents">Return to the table of contents</a>
        </details>
        </section>
        <section id="PROCESS ADDRESS SPACE">
            <details>
                <summary>PROCESS ADDRESS SPACE</summary>
                <p>As seen in the previous chapter, a kernel function gets dynamic memory in a fairly
                    straightforward manner by invoking one of a variety of functions: __get_free_pages( )
                    or alloc_pages() to get pages from the zoned page frame allocator, kmem_cache_
                    alloc( ) or kmalloc( ) to use the slab allocator for specialized or general-purpose
                    objects, and vmalloc( ) or vmalloc_32() to get a noncontiguous memory area. If the
                    request can be satisfied, each of these functions returns a page descriptor address or a
                    linear address identifying the beginning of the allocated dynamic memory area.
                    These simple approaches work for two reasons:
                    • The kernel is the highest-priority component of the operating system. If a kernel
                    function makes a request for dynamic memory, it must have a valid reason to
                    issue that request, and there is no point in trying to defer it.
                    • The kernel trusts itself. All kernel functions are assumed to be error-free, so the
                    kernel does not need to insert any protection against programming errors.
                    When allocating memory to User Mode processes, the situation is entirely different:
                    • Process requests for dynamic memory are considered non-urgent. When a process’s
                    executable file is loaded, for instance, it is unlikely that the process will
                    address all the pages of code in the near future. Similarly, when a process
                    invokes malloc( ) to get additional dynamic memory, it doesn’t mean the process
                    will soon access all the additional memory obtained. Thus, as a general rule,
                    the kernel tries to defer allocating dynamic memory to User Mode processes.
                    • Because user programs cannot be trusted, the kernel must be prepared to catch
                    all addressing errors caused by processes in User Mode.
                    As this chapter describes, the kernel succeeds in deferring the allocation of dynamic
                    memory to processes by using a new kind of resource. When a User Mode process
                    asks for dynamic memory, it doesn’t get additional page frames; instead, it gets the
                    right to use a new range of linear addresses, which become part of its address space.
                    This interval is called a “memory region.”In the next section, we discuss how the process views
                    dynamic memory. We then
                    describe the basic components of the process address space in the section “Memory
                    Regions.” Next, we examine in detail the role played by the Page Fault exception
                    handler in deferring the allocation of page frames to processes and illustrate how the
                    kernel creates and deletes whole process address spaces. Last, we discuss the APIs
                    and system calls related to address space management</p>
                <a href="#Table of contents">Return to the table of contents</a>
            </details>
        </section>
        <section id="SYSTEM CALLS">
            <details>
                <summary>SYSTEM CALLS</summary>
                <p>Operating systems offer processes running in User Mode a set of interfaces to interact
                    with hardware devices such as the CPU, disks, and printers. Putting an extra
                    layer between the application and the hardware has several advantages. First, it
                    makes programming easier by freeing users from studying low-level programming
                    characteristics of hardware devices. Second, it greatly increases system security,
                    because the kernel can check the accuracy of the request at the interface level before
                    attempting to satisfy it. Last but not least, these interfaces make programs more portable,
                    because they can be compiled and executed correctly on every kernel that
                    offers the same set of interfaces.
                    Unix systems implement most interfaces between User Mode processes and hardware
                    devices by means of system calls issued to the kernel. This chapter examines in detail
                    how Linux implements system calls that User Mode programs issue to the kernel.</p>
                <a href="#Table of contents">Return to the table of contents</a>
            </details>
        </section>
        <section id="SIGNALS">
            <details>
                <summary>SIGNALS</summary>
                <p>Signals were introduced by the first Unix systems to allow interactions between User
                    Mode processes; the kernel also uses them to notify processes of system events. Signals
                    have been around for 30 years with only minor changes.
                    The first sections of this chapter examine in detail how signals are handled by the
                    Linux kernel, then we discuss the system calls that allow processes to exchange signals.</p>
                <a href="#Table of contents">Return to the table of contents</a>
            </details>
        </section>
        <section id="THE VIRTUAL FILE SYSTEM">
            <details>
                <summary>THE VIRTUAL FILE SYSTEM</summary>
                <p>One of Linux’s keys to success is its ability to coexist comfortably with other systems.
                    You can transparently mount disks or partitions that host file formats used by
                    Windows, other Unix systems, or even systems with tiny market shares like the
                    Amiga. Linux manages to support multiple filesystem types in the same way other
                    Unix variants do, through a concept called the Virtual Filesystem.
                    The idea behind the Virtual Filesystem is to put a wide range of information in the
                    kernel to represent many different types of filesystems; there is a field or function to
                    support each operation provided by all real filesystems supported by Linux. For each
                    read, write, or other function called, the kernel substitutes the actual function that
                    supports a native Linux filesystem, the NTFS filesystem, or whatever other filesystem
                    the file is on.
                    This chapter discusses the aims, structure, and implementation of Linux’s Virtual
                    Filesystem. It focuses on three of the five standard Unix file types—namely, regular
                    files, directories, and symbolic links. Device files are covered in Chapter 13, while
                    pipes are discussed in Chapter 19. To show how a real filesystem works, Chapter 18
                    covers the Second Extended Filesystem that appears on nearly all Linux systems.</p>
                <a href="#Table of contents">Return to the table of contents</a>
            </details>
        </section>
        <section id="I/O ARCHITECTURE AND DEVICE DRIVERS">
            <details>
                <summary>I/O ARCHITECTURE AND DEVICE DRIVERS</summary>
                <p>The Virtual File System in the last chapter depends on lower-level functions to carry
                    out each read, write, or other operation in a manner suited to each device. The previous
                    chapter included a brief discussion of how operations are handled by different
                    filesystems. In this chapter, we look at how the kernel invokes the operations on
                    actual devices.
                    In the section “I/O Architecture,” we give a brief survey of the 80 × 86 I/O architecture.
                    In the section “The Device Driver Model,” we introduce the Linux device driver
                    model. Next, in the section “Device Files,” we show how the VFS associates a special
                    file called “device file” with each different hardware device, so that application
                    programs can use all kinds of devices in the same way. We then introduce in the section
                    “Device Drivers” some common characteristics of device drivers. Finally, in the
                    section “Character Device Drivers,” we illustrate the overall organization of character
                    device drivers in Linux. We’ll defer the discussion of block device drivers to the
                    next chapters.
                    Readers interested in developing device drivers on their own may want to refer to
                    Jonathan Corbet, Alessandro Rubini, and Greg Kroah-Hartman’s Linux Device Drivers,
                    Third Edition (O’Reilly).</p>
                <a href="#Table of contents">Return to the table of contents</a>
            </details>
        </section>
        <section id="BLOCK DEVICE DRIVERS">
            <details>
                <summary>BLOCK DEVICE DRIVERS</summary>
                <p>This chapter deals with I/O drivers for block devices, i.e., for disks of every kind. The
                    key aspect of a block device is the disparity between the time taken by the CPU and
                    buses to read or write data and the speed of the disk hardware. Block devices have
                    very high average access times. Each operation requires several milliseconds to complete,
                    mainly because the disk controller must move the heads on the disk surface to
                    reach the exact position where the data is recorded. However, when the heads are correctly
                    placed, data transfer can be sustained at rates of tens of megabytes per second.
                    The organization of Linux block device handlers is quite involved. We won’t be able
                    to discuss in detail all the functions that are included in the block I/O subsystem of
                    the kernel; however, we’ll outline the general software architecture. As in the previous
                    chapter, our objective is to explain how Linux supports the implementation of
                    block device drivers, rather than showing how to implement one of them.
                    We start in the first section “Block Devices Handling” to explain the general architecture
                    of the Linux block I/O subsystem. In the sections “The Generic Block Layer,”
                    “The I/O Scheduler,” and “Block Device Drivers,” we will describe the main components
                    of the block I/O subsystem. Finally, in the last section, “Opening a Block
                    Device File,” we will outline the steps performed by the kernel when opening a block
                    device file.</p>
                <a href="#Table of contents">Return to the table of contents</a>
            </details>
        </section>
        <section id="THE PAGE CACHE">
            <details>
                <summary>THE PAGE CACHE</summary>
                <p>As already mentioned in the section “The Common File Model” in Chapter 12, a
                    disk cache is a software mechanism that allows the system to keep in RAM some
                    data that is normally stored on a disk, so that further accesses to that data can be satisfied
                    quickly without accessing the disk.
                    Disk caches are crucial for system performance, because repeated accesses to the
                    same disk data are quite common. A User Mode process that interacts with a disk is
                    entitled to ask repeatedly to read or write the same disk data. Moreover, different
                    processes may also need to address the same disk data at different times. As an
                    example, you may use the cp command to copy a text file and then invoke your
                    favorite editor to modify it. To satisfy your requests, the command shell will create
                    two different processes that access the same file at different times.
                    We have already encountered other disk caches in Chapter 12: the dentry cache,
                    which stores dentry objects representing filesystem pathnames, and the inode cache,
                    which stores inode objects representing disk inodes. Notice, however, that dentry
                    objects and inode objects are not mere buffers storing the contents of some disk
                    blocks; thus, the dentry cache and the inode cache are rather peculiar as disk caches.
                    This chapter deals with the page cache, which is a disk cache working on whole
                    pages of data. We introduce the page cache in the first section. Then, we discuss in
                    the section “Storing Blocks in the Page Cache” how the page cache can be used to
                    retrieve single blocks of data (for instance, superblocks and inodes); this feature is
                    crucial to speed up the VFS and the disk-based filesystems. Next, we describe in the
                    section “Writing Dirty Pages to Disk” how the dirty pages in the page cache are written
                    back to disk. Finally, we mention in the last section “The sync( ), fsync( ), and
                    fdatasync() System Calls” some system calls that allow a user to flush the contents of
                    the page cache so as to update the disk contents.</p>
                <a href="#Table of contents">Return to the table of contents</a>
            </details>
        </section>
        <section id="ACCESSING FILES">
            <details>
                <summary>ACCESSING FILES</summary>
                <p> Accessing a disk-based file is a complex activity that involves the VFS abstraction
                    layer (Chapter 12), handling block devices (Chapter 14), and the use of the page
                    cache (Chapter 15). This chapter shows how the kernel builds on all those facilities
                    to carry out file reads and writes. The topics covered in this chapter apply both to
                    regular files stored in disk-based filesystems and to block device files; these two
                    kinds of files will be referred to simply as “files.”
                    The stage we are working at in this chapter starts after the proper read or write
                    method of a particular file has been called (as described in Chapter 12). We show
                    here how each read ends with the desired data delivered to a User Mode process and
                    how each write ends with data marked ready for transfer to disk. The rest of the
                    transfer is handled by the facilities described in Chapter 14 and Chapter 15.
                    There are many different ways to access a file. In this chapter we will consider the
                    following cases:
                    Canonical mode
                    The file is opened with the O_SYNC and O_DIRECT flags cleared, and its content is
                    accessed by means of the read() and write() system calls. In this case, the read(
                    ) system call blocks the calling process until the data is copied into the User
                    Mode address space (however, the kernel is always allowed to return fewer bytes
                    than requested!). The write() system call is different, because it terminates as
                    soon as the data is copied into the page cache (deferred write). This case is covered
                    in the section “Reading and Writing a File.”
                    Synchronous mode
                    The file is opened with the O_SYNC flag set—or the flag is set at a later time by the
                    fcntl() system call. This flag affects only the write operation (read operations are
                    always blocking), which blocks the calling process until the data is effectively
                    written to disk. The section “Reading and Writing a File” covers this case, too.
                    Memory mapping mode
                    After opening the file, the application issues an mmap() system call to map the file
                    into memory. As a result, the file appears as an array of bytes in RAM, and the
                    application accesses directly the array elements instead of using read(), write(),
                    or lseek(). This case is discussed in the section “Memory Mapping.”
                    Direct I/O mode
                    The file is opened with the O_DIRECT flag set. Any read or write operation transfers
                    data directly from the User Mode address space to disk, or vice versa,
                    bypassing the page cache. We discuss this case in the section “Direct I/O Transfers.”
                    (The values of the O_SYNC and O_DIRECT flags can be combined in four
                    meaningful ways.)
                    Asynchronous mode
                    The file is accessed—either through a group of POSIX APIs or by means of
                    Linux-specific system calls—in such a way to perform “asynchronous I/O:” this
                    means the requests for data transfers never block the calling process; rather, they
                    are carried on “in the background” while the application continues its normal
                    execution. We discuss this case in the section “Asynchronous I/O.”</p>
                <a href="#Table of contents">Return to the table of contents</a>
            </details>
        </section>
        <section id="PAGE FRAME RECLAIMING">
            <details>
                <summary>PAGE FRAME RECLAIMING</summary>
                <p>In previous chapters, we explained how the kernel handles dynamic memory by
                    keeping track of free and busy page frames. We have also discussed how every process
                    in User Mode has its own address space and has its requests for memory satisfied
                    by the kernel one page at a time, so that page frames can be assigned to the
                    process at the very last possible moment. Last but not least, we have shown how the
                    kernel makes use of dynamic memory to implement both memory and disk caches.
                    In this chapter, we complete our description of the virtual memory subsystem by discussing
                    page frame reclaiming. We’ll start in the first section, “The Page Frame
                    Reclaiming Algorithm,” explaining why the kernel needs to reclaim page frames and
                    what strategy it uses to achieve this. We then make a technical digression in the section
                    “Reverse Mapping” to discuss the data structures used by the kernel to locate
                    quickly all the Page Table entries that point to the same page frame. The section
                    “Implementing the PFRA” is devoted to the page frame reclaiming algorithm used by
                    Linux. The last main section, “Swapping,” is almost a chapter by itself: it covers the
                    swap subsystem, a kernel component used to save anonymous (not mapping data of
                    files) pages on disk.</p>
                <a href="#Table of contents">Return to the table of contents</a>
            </details>
        </section>
        <section id="THE EXT2 AND EXT3 FILESYSTEMS">
            <details>
                <summary>THE EXT2 AND EXT3 FILESYSTEMS</summary>
                <p>In this chapter, we finish our extensive discussion of I/O and filesystems by taking a
                    look at the details the kernel has to take care of when interacting with a specific filesystem.
                    Because the Second Extended Filesystem (Ext2) is native to Linux and is
                    used on virtually every Linux system, it is a natural choice for this discussion. Furthermore,
                    Ext2 illustrates a lot of good practices in its support for modern filesystem
                    features with fast performance. To be sure, other filesystems supported by Linux
                    include many interesting features, but we have no room to examine all of them.
                    After introducing Ext2 in the section “General Characteristics of Ext2,” we describe
                    the data structures needed, just as in other chapters. Because we are looking at a specific
                    way to store data on disk, we have to consider two versions of the same data
                    structures. The section “Ext2 Disk Data Structures” shows the data structures stored
                    by Ext2 on disk, while “Ext2 Memory Data Structures” shows the corresponding versions
                    in memory.
                    Then we get to the operations performed on the filesystem. In the section “Creating
                    the Ext2 Filesystem,” we discuss how Ext2 is created in a disk partition. The next
                    sections describe the kernel activities performed whenever the disk is used. Most of
                    these are relatively low-level activities dealing with the allocation of disk space to
                    inodes and data blocks.
                    In the last section, we give a short description of the Ext3 filesystem, which is the
                    next step in the evolution of the Ext2 filesystem.</p>
                <a href="#Table of contents">Return to the table of contents</a>
            </details>
        </section>
        <section id="PROCESS COMMUNICATION">
            <details>
                <summary>PROCESS COMMUNICATION</summary>
                <p>This chapter explains how User Mode processes can synchronize their actions and
                    exchange data. We already covered several synchronization topics in Chapter 5, but
                    the actors there were kernel control paths, not User Mode programs. We are now
                    ready, after having discussed I/O management and filesystems at length, to extend
                    the discussion to User Mode processes. These processes must rely on the kernel to
                    facilitate interprocess synchronization and communication.
                    As we saw in the section “Linux File Locking” in Chapter 12, a form of synchronization
                    among User Mode processes can be achieved by creating a (possibly empty) file
                    and using suitable VFS system calls to lock and unlock it. While processes can similarly
                    share data via temporary files protected by locks, this approach is costly because
                    it requires accesses to the filesystem on disk. For this reason, all Unix kernels include
                    a set of system calls that supports process communication without interacting with
                    the filesystem; furthermore, several wrapper functions were developed and inserted
                    in suitable libraries to expedite how processes issue their synchronization requests to
                    the kernel.
                    As usual, application programmers have a variety of needs that call for different communication
                    mechanisms. Here are the basic mechanisms that Unix systems offer to
                    allow interprocess communication:
                    Pipes and FIFOs (named pipes)
                    Best suited to implement producer/consumer interactions among processes.
                    Some processes fill the pipe with data, while others extract data from the pipe.
                    They are covered in the sections “Pipes” and “FIFOs.”
                    Semaphores
                    Represent, as the name implies, the User Mode version of the kernel semaphores
                    discussed in the section “Semaphores” in Chapter 5. They are described
                    in the section “System V IPC.” Messages
                    Allow processes to exchange messages (short blocks of data) by reading and
                    writing them in predefined message queues. The Linux kernel offers two different
                    versions of messages: System V IPC messages (covered in the section “System
                    V IPC”) and POSIX messages (described in the section “POSIX Message
                    Queues”).
                    Shared memory regions
                    Allow processes to exchange information via a shared block of memory. In applications
                    that must share large amounts of data, this can be the most efficient form
                    of process communication. They are described in the section “System V IPC.”
                    Sockets
                    Allow processes on different computers to exchange data through a network.
                    Sockets can also be used as a communication tool for processes located on the
                    same host computer; the X Window System graphic interface, for instance, uses
                    a socket to allow client programs to exchange data with the X server</p>
                <a href="#Table of contents">Return to the table of contents</a>
            </details>
        </section>
        <section id="PROGRAM EXECUTION">
            <details>
                <summary>PROGRAM EXECUTION</summary>
                <p>The concept of a “process,” described in Chapter 3, was used in Unix from the
                    beginning to represent the behavior of groups of running programs that compete for
                    system resources. This final chapter focuses on the relationship between program
                    and process. We specifically describe how the kernel sets up the execution context
                    for a process according to the contents of the program file. While it may not seem
                    like a big problem to load a bunch of instructions into memory and point the CPU to
                    them, the kernel has to deal with flexibility in several areas:
                    Different executable formats
                    Linux is distinguished by its ability to run binaries that were compiled for other
                    operating systems. In particular, Linux is able to run an executable created for a
                    32-bit machine on the 64-bit version of the same machine. For instance, an executable
                    created on a Pentium can run on a 64-bit AMD Opteron.
                    Shared libraries
                    Many executable files don’t contain all the code required to run the program but
                    expect the kernel to load in functions from a library at runtime.
                    Other information in the execution context
                    This includes the command-line arguments and environment variables familiar
                    to programmers.
                    A program is stored on disk as an executable file, which includes both the object code
                    of the functions to be executed and the data on which these functions will act. Many
                    functions of the program are service routines available to all programmers; their
                    object code is included in special files called “libraries.” Actually, the code of a
                    library function may either be statically copied into the executable file (static libraries)
                    or linked to the process at runtime (shared libraries, because their code can be
                    shared by several independent processes).
                    When launching a program, the user may supply two kinds of information that affect
                    the way it is executed: command-line arguments and environment variables. Command-
                    line arguments are typed in by the user following the executable filename at the shell prompt.
                    Environment variables, such as HOME and PATH, are inherited from the
                    shell, but the users may modify the values of such variables before they launch the
                    program.
                    In the section “Executable Files,” we explain what a program execution context is. In
                    the section “Executable Formats,” we mention some of the executable formats supported
                    by Linux and show how Linux can change its “personality” to execute programs
                    compiled for other operating systems. Finally, in the section “The exec
                    Functions,” we describe the system call that allows a process to start executing a new
                    program</p>
                <a href="#Table of contents">Return to the table of contents </a>
            </details>
        </section>
        <section id="SYSTEM STARTUP">
            <details>
                <summary>SYSTEM STARTUP</summary>
                <p>This appendix explains what happens right after users switch on their computers—
                    that is, how a Linux kernel image is copied into memory and executed. In short, we
                    discuss how the kernel, and thus the whole system, is “bootstrapped.”
                    Traditionally, the term bootstrap refers to a person who tries to stand up by pulling
                    his own boots. In operating systems, the term denotes bringing at least a portion of
                    the operating system into main memory and having the processor execute it. It also
                    denotes the initialization of kernel data structures, the creation of some user processes,
                    and the transfer of control to one of them.
                    Computer bootstrapping is a tedious, long task, because initially, nearly every hardware
                    device, including the RAM, is in a random, unpredictable state. Moreover, the
                    bootstrap process is highly dependent on the computer architecture; as usual in this
                    book, we refer to the 80 × 86 architecture.</p>
                <a href="#Table of contents">Return to the table of contents </a>
            </details>
        </section>
        <section id="MODULES">
            <details>
                <summary>MODULES</summary>
                <p>As stated in Chapter 1, modules are Linux’s recipe for effectively achieving many of the
                    theoretical advantages of microkernels without introducing performance penalties.</p>
                <a href="#Table of contents">Return to the table of contents </a>
            </details>
        </section>
        <section id="BIBLIOGRAPHY">
            <details>
                <summary>BIBLIOGRAPHY</summary>
                <p>This bibliography is broken down by subject area and lists some of the most common
                    and, in our opinion, useful books and online documentation on the topic of
                    kernels.</p>
                <a href="#Table of contents">Return to the table of contents </a>
            </details>
        </section>
       
    </main>

    <footer>
        <div id="upper-footer">
            <h3>Catch up with WideMind community</h3>
            <ul class="menu">
                <li><a href="#">Github</a></li>
                <li><a href="#">LinkedIn</a></li>
                <li><a href="#">X</a></li>
            </ul>
        </div>
        <div id="middle-footer">
            <h3>Contact Us</h3>
            <ul class="menu">
                <li>
                    <address>
                        Patision 76<br />
                        Zip-Code 104 34 Athens
                        <b><a href="#">Find in map</a></b>
                    </address>
                </li>
                <li>Tel: 210-8203315,316</li>
                <li><a href="mailto:widemind@gmail.com">widemind@gmail.com</a></li>
            </ul>
        </div>
        <div id="lower-footer">
            <p>©Copyright 2024 WideMind Organization</p>
        </div>
    </footer>
</body>

</html>